{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be8e9c9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-16T17:58:54.043256Z",
     "iopub.status.busy": "2026-02-16T17:58:54.042246Z",
     "iopub.status.idle": "2026-02-16T17:59:03.619939Z",
     "shell.execute_reply": "2026-02-16T17:59:03.618928Z"
    },
    "papermill": {
     "duration": 9.584402,
     "end_time": "2026-02-16T17:59:03.622242",
     "exception": false,
     "start_time": "2026-02-16T17:58:54.037840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.8.0+cu126.html\r\n",
      "Collecting torch_geometric\r\n",
      "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyg_lib\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/pyg_lib-0.5.0%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (4.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch_scatter\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_scatter-2.1.2%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (10.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch_sparse\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_sparse-0.6.18%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (5.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch_cluster\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_cluster-1.6.3%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torch_spline_conv\r\n",
      "  Downloading https://data.pyg.org/whl/torch-2.8.0%2Bcu126/torch_spline_conv-1.2.2%2Bpt28cu126-cp312-cp312-linux_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.3)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.10.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.5)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch_sparse) (1.15.3)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.6.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2026.1.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\r\n",
      "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster, torch_geometric\r\n",
      "Successfully installed pyg_lib-0.5.0+pt28cu126 torch_cluster-1.6.3+pt28cu126 torch_geometric-2.7.0 torch_scatter-2.1.2+pt28cu126 torch_sparse-0.6.18+pt28cu126 torch_spline_conv-1.2.2+pt28cu126\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.8.0+cu126.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58f3875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:59:03.630956Z",
     "iopub.status.busy": "2026-02-16T17:59:03.630465Z",
     "iopub.status.idle": "2026-02-16T17:59:22.880829Z",
     "shell.execute_reply": "2026-02-16T17:59:22.879735Z"
    },
    "papermill": {
     "duration": 19.257341,
     "end_time": "2026-02-16T17:59:22.883082",
     "exception": false,
     "start_time": "2026-02-16T17:59:03.625741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import pandas as pd\n",
    "\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff5d40c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:59:22.891753Z",
     "iopub.status.busy": "2026-02-16T17:59:22.891021Z",
     "iopub.status.idle": "2026-02-16T17:59:22.901308Z",
     "shell.execute_reply": "2026-02-16T17:59:22.899967Z"
    },
    "papermill": {
     "duration": 0.017024,
     "end_time": "2026-02-16T17:59:22.903447",
     "exception": false,
     "start_time": "2026-02-16T17:59:22.886423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GIN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(\n",
    "                GINConv(nn.Sequential(\n",
    "                    nn.Linear(input_dim, 2 * hidden_dim),\n",
    "                    nn.BatchNorm1d(2 * hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "                ))\n",
    "            )\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        self.lin1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv, batch_norm in zip(self.convs, self.batch_norms):\n",
    "            x = F.relu(batch_norm(conv(x, edge_index)))\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.batch_norm1(self.lin1(x)))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return self.classifier(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea40a48b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:59:22.911920Z",
     "iopub.status.busy": "2026-02-16T17:59:22.911060Z",
     "iopub.status.idle": "2026-02-16T17:59:22.919075Z",
     "shell.execute_reply": "2026-02-16T17:59:22.917992Z"
    },
    "papermill": {
     "duration": 0.014323,
     "end_time": "2026-02-16T17:59:22.921032",
     "exception": false,
     "start_time": "2026-02-16T17:59:22.906709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = (out > 0).float()\n",
    "        predictions.append(pred.cpu())\n",
    "        labels.append(data.y.cpu())\n",
    "\n",
    "    accuracy = metrics.accuracy_score(torch.cat(labels), torch.cat(predictions))\n",
    "    f1 = metrics.f1_score(torch.cat(labels), torch.cat(predictions))\n",
    "\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263ecd64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:59:22.929047Z",
     "iopub.status.busy": "2026-02-16T17:59:22.928688Z",
     "iopub.status.idle": "2026-02-16T17:59:22.938710Z",
     "shell.execute_reply": "2026-02-16T17:59:22.937799Z"
    },
    "papermill": {
     "duration": 0.016345,
     "end_time": "2026-02-16T17:59:22.940708",
     "exception": false,
     "start_time": "2026-02-16T17:59:22.924363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"GIN for partial automorphism extension problem\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, \n",
    "                    help=\"Random seed for reproducibility (default: 42)\") \n",
    "parser.add_argument(\"--batch_size\", type=int, default=64,\n",
    "                    help=\"Input batch size (default: 64)\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=150,\n",
    "                    help=\"Number of epochs to train (default: 150)\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0008007016085176578,\n",
    "                    help=\"Learning rate (default: 0.0008007016085176578)\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=1.5408221478908417e-05,\n",
    "                    help=\"Weight decay (default: 1.5408221478908417e-05)\")\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=512,\n",
    "                    help=\"Hidden dimension size (default: 512)\") \n",
    "parser.add_argument(\"--num_layers\", type=int, default=2, \n",
    "                    help=\"Number of GIN layers (default: 2)\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.04821922755593036, \n",
    "                    help=\"Dropout rate (default: 0.04821922755593036)\")\n",
    "parser.add_argument(\"--factor\", type=float, default=0.5,\n",
    "                    help=\"Factor for learning rate scheduler (default: 0.5)\")\n",
    "parser.add_argument(\"--patience\", type=int, default=3,\n",
    "                    help=\"Patience for learning rate scheduler (default: 3)\")\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579ec018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:59:22.948631Z",
     "iopub.status.busy": "2026-02-16T17:59:22.948305Z",
     "iopub.status.idle": "2026-02-16T17:59:35.287802Z",
     "shell.execute_reply": "2026-02-16T17:59:35.286686Z"
    },
    "papermill": {
     "duration": 12.346104,
     "end_time": "2026-02-16T17:59:35.290086",
     "exception": false,
     "start_time": "2026-02-16T17:59:22.943982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_geometric.seed_everything(args.seed)\n",
    "\n",
    "train_dataset = torch.load('/kaggle/input/graphs-with-automorphisms/train_dataset.pt',weights_only=False)\n",
    "val_dataset = torch.load('/kaggle/input/graphs-with-automorphisms/val_dataset.pt',weights_only=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = GIN(3, args.hidden_dim, args.num_layers, args.dropout).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=args.factor, patience=args.patience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de2d57b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T17:59:35.298216Z",
     "iopub.status.busy": "2026-02-16T17:59:35.297837Z",
     "iopub.status.idle": "2026-02-16T22:09:20.604299Z",
     "shell.execute_reply": "2026-02-16T22:09:20.603294Z"
    },
    "papermill": {
     "duration": 14985.319953,
     "end_time": "2026-02-16T22:09:20.613122",
     "exception": false,
     "start_time": "2026-02-16T17:59:35.293169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train Loss: 0.6431 | Train Acc: 0.6769 | Train F1:  0.7127 | Val Acc:   0.6758 | Val F1:    0.7096\n",
      "Epoch 02 | Train Loss: 0.5782 | Train Acc: 0.7073 | Train F1:  0.7408 | Val Acc:   0.7030 | Val F1:    0.7368\n",
      "Epoch 03 | Train Loss: 0.5553 | Train Acc: 0.7267 | Train F1:  0.7725 | Val Acc:   0.7247 | Val F1:    0.7708\n",
      "Epoch 04 | Train Loss: 0.5380 | Train Acc: 0.7301 | Train F1:  0.7644 | Val Acc:   0.7283 | Val F1:    0.7619\n",
      "Epoch 05 | Train Loss: 0.5272 | Train Acc: 0.7400 | Train F1:  0.7886 | Val Acc:   0.7369 | Val F1:    0.7867\n",
      "Epoch 06 | Train Loss: 0.5160 | Train Acc: 0.7505 | Train F1:  0.7924 | Val Acc:   0.7505 | Val F1:    0.7925\n",
      "Epoch 07 | Train Loss: 0.5063 | Train Acc: 0.7531 | Train F1:  0.7923 | Val Acc:   0.7505 | Val F1:    0.7889\n",
      "Epoch 08 | Train Loss: 0.4984 | Train Acc: 0.7563 | Train F1:  0.7990 | Val Acc:   0.7522 | Val F1:    0.7952\n",
      "Epoch 09 | Train Loss: 0.4925 | Train Acc: 0.7644 | Train F1:  0.8015 | Val Acc:   0.7621 | Val F1:    0.7990\n",
      "Epoch 10 | Train Loss: 0.4874 | Train Acc: 0.7638 | Train F1:  0.7913 | Val Acc:   0.7596 | Val F1:    0.7873\n",
      "Epoch 11 | Train Loss: 0.4817 | Train Acc: 0.7642 | Train F1:  0.7987 | Val Acc:   0.7630 | Val F1:    0.7973\n",
      "Epoch 12 | Train Loss: 0.4754 | Train Acc: 0.7723 | Train F1:  0.8026 | Val Acc:   0.7648 | Val F1:    0.7949\n",
      "Epoch 13 | Train Loss: 0.4713 | Train Acc: 0.7700 | Train F1:  0.8060 | Val Acc:   0.7678 | Val F1:    0.8041\n",
      "Epoch 14 | Train Loss: 0.4662 | Train Acc: 0.7726 | Train F1:  0.7961 | Val Acc:   0.7630 | Val F1:    0.7874\n",
      "Epoch 15 | Train Loss: 0.4625 | Train Acc: 0.7818 | Train F1:  0.8118 | Val Acc:   0.7715 | Val F1:    0.8027\n",
      "Epoch 16 | Train Loss: 0.4563 | Train Acc: 0.7776 | Train F1:  0.8021 | Val Acc:   0.7681 | Val F1:    0.7926\n",
      "Epoch 17 | Train Loss: 0.4518 | Train Acc: 0.7921 | Train F1:  0.8202 | Val Acc:   0.7804 | Val F1:    0.8100\n",
      "Epoch 18 | Train Loss: 0.4498 | Train Acc: 0.7913 | Train F1:  0.8286 | Val Acc:   0.7812 | Val F1:    0.8202\n",
      "Epoch 19 | Train Loss: 0.4455 | Train Acc: 0.7928 | Train F1:  0.8215 | Val Acc:   0.7780 | Val F1:    0.8083\n",
      "Epoch 20 | Train Loss: 0.4408 | Train Acc: 0.7998 | Train F1:  0.8238 | Val Acc:   0.7872 | Val F1:    0.8129\n",
      "Epoch 21 | Train Loss: 0.4379 | Train Acc: 0.7938 | Train F1:  0.8216 | Val Acc:   0.7807 | Val F1:    0.8103\n",
      "Epoch 22 | Train Loss: 0.4339 | Train Acc: 0.8030 | Train F1:  0.8342 | Val Acc:   0.7886 | Val F1:    0.8219\n",
      "Epoch 23 | Train Loss: 0.4305 | Train Acc: 0.8057 | Train F1:  0.8382 | Val Acc:   0.7890 | Val F1:    0.8245\n",
      "Epoch 24 | Train Loss: 0.4269 | Train Acc: 0.8104 | Train F1:  0.8409 | Val Acc:   0.7917 | Val F1:    0.8261\n",
      "Epoch 25 | Train Loss: 0.4227 | Train Acc: 0.8101 | Train F1:  0.8355 | Val Acc:   0.7910 | Val F1:    0.8190\n",
      "Epoch 26 | Train Loss: 0.4208 | Train Acc: 0.8175 | Train F1:  0.8443 | Val Acc:   0.7943 | Val F1:    0.8241\n",
      "Epoch 27 | Train Loss: 0.4157 | Train Acc: 0.8124 | Train F1:  0.8449 | Val Acc:   0.7947 | Val F1:    0.8304\n",
      "Epoch 28 | Train Loss: 0.4131 | Train Acc: 0.8163 | Train F1:  0.8441 | Val Acc:   0.7956 | Val F1:    0.8262\n",
      "Epoch 29 | Train Loss: 0.4094 | Train Acc: 0.8221 | Train F1:  0.8477 | Val Acc:   0.8019 | Val F1:    0.8306\n",
      "Epoch 30 | Train Loss: 0.4065 | Train Acc: 0.8274 | Train F1:  0.8497 | Val Acc:   0.7959 | Val F1:    0.8229\n",
      "Epoch 31 | Train Loss: 0.4029 | Train Acc: 0.8212 | Train F1:  0.8416 | Val Acc:   0.7887 | Val F1:    0.8130\n",
      "Epoch 32 | Train Loss: 0.4006 | Train Acc: 0.8286 | Train F1:  0.8511 | Val Acc:   0.7985 | Val F1:    0.8248\n",
      "Epoch 33 | Train Loss: 0.3966 | Train Acc: 0.8243 | Train F1:  0.8523 | Val Acc:   0.7982 | Val F1:    0.8306\n",
      "Epoch 34 | Train Loss: 0.3787 | Train Acc: 0.8440 | Train F1:  0.8649 | Val Acc:   0.8049 | Val F1:    0.8311\n",
      "Epoch 35 | Train Loss: 0.3750 | Train Acc: 0.8412 | Train F1:  0.8643 | Val Acc:   0.8040 | Val F1:    0.8331\n",
      "Epoch 36 | Train Loss: 0.3715 | Train Acc: 0.8453 | Train F1:  0.8663 | Val Acc:   0.8079 | Val F1:    0.8345\n",
      "Epoch 37 | Train Loss: 0.3664 | Train Acc: 0.8552 | Train F1:  0.8730 | Val Acc:   0.8080 | Val F1:    0.8318\n",
      "Epoch 38 | Train Loss: 0.3634 | Train Acc: 0.8460 | Train F1:  0.8664 | Val Acc:   0.8029 | Val F1:    0.8300\n",
      "Epoch 39 | Train Loss: 0.3612 | Train Acc: 0.8553 | Train F1:  0.8742 | Val Acc:   0.8055 | Val F1:    0.8316\n",
      "Epoch 40 | Train Loss: 0.3594 | Train Acc: 0.8537 | Train F1:  0.8739 | Val Acc:   0.8072 | Val F1:    0.8345\n",
      "Epoch 41 | Train Loss: 0.3568 | Train Acc: 0.8560 | Train F1:  0.8766 | Val Acc:   0.8078 | Val F1:    0.8363\n",
      "Epoch 42 | Train Loss: 0.3439 | Train Acc: 0.8677 | Train F1:  0.8829 | Val Acc:   0.8110 | Val F1:    0.8335\n",
      "Epoch 43 | Train Loss: 0.3429 | Train Acc: 0.8640 | Train F1:  0.8824 | Val Acc:   0.8127 | Val F1:    0.8393\n",
      "Epoch 44 | Train Loss: 0.3387 | Train Acc: 0.8705 | Train F1:  0.8863 | Val Acc:   0.8085 | Val F1:    0.8330\n",
      "Epoch 45 | Train Loss: 0.3376 | Train Acc: 0.8727 | Train F1:  0.8875 | Val Acc:   0.8090 | Val F1:    0.8318\n",
      "Epoch 46 | Train Loss: 0.3347 | Train Acc: 0.8734 | Train F1:  0.8891 | Val Acc:   0.8124 | Val F1:    0.8368\n",
      "Epoch 47 | Train Loss: 0.3341 | Train Acc: 0.8702 | Train F1:  0.8879 | Val Acc:   0.8128 | Val F1:    0.8397\n",
      "Epoch 48 | Train Loss: 0.3257 | Train Acc: 0.8777 | Train F1:  0.8923 | Val Acc:   0.8108 | Val F1:    0.8345\n",
      "Epoch 49 | Train Loss: 0.3240 | Train Acc: 0.8782 | Train F1:  0.8937 | Val Acc:   0.8132 | Val F1:    0.8381\n",
      "Epoch 50 | Train Loss: 0.3245 | Train Acc: 0.8755 | Train F1:  0.8918 | Val Acc:   0.8122 | Val F1:    0.8383\n",
      "Epoch 51 | Train Loss: 0.3207 | Train Acc: 0.8762 | Train F1:  0.8929 | Val Acc:   0.8127 | Val F1:    0.8393\n",
      "Epoch 52 | Train Loss: 0.3202 | Train Acc: 0.8798 | Train F1:  0.8951 | Val Acc:   0.8148 | Val F1:    0.8398\n",
      "Epoch 53 | Train Loss: 0.3215 | Train Acc: 0.8818 | Train F1:  0.8961 | Val Acc:   0.8121 | Val F1:    0.8363\n",
      "Epoch 54 | Train Loss: 0.3194 | Train Acc: 0.8807 | Train F1:  0.8959 | Val Acc:   0.8116 | Val F1:    0.8369\n",
      "Epoch 55 | Train Loss: 0.3156 | Train Acc: 0.8864 | Train F1:  0.8994 | Val Acc:   0.8139 | Val F1:    0.8360\n",
      "Epoch 56 | Train Loss: 0.3152 | Train Acc: 0.8849 | Train F1:  0.8987 | Val Acc:   0.8127 | Val F1:    0.8364\n",
      "Epoch 57 | Train Loss: 0.3118 | Train Acc: 0.8849 | Train F1:  0.8991 | Val Acc:   0.8128 | Val F1:    0.8371\n",
      "Epoch 58 | Train Loss: 0.3097 | Train Acc: 0.8836 | Train F1:  0.8986 | Val Acc:   0.8130 | Val F1:    0.8385\n",
      "Epoch 59 | Train Loss: 0.3111 | Train Acc: 0.8868 | Train F1:  0.9000 | Val Acc:   0.8120 | Val F1:    0.8351\n",
      "Epoch 60 | Train Loss: 0.3094 | Train Acc: 0.8837 | Train F1:  0.8989 | Val Acc:   0.8122 | Val F1:    0.8385\n",
      "Epoch 61 | Train Loss: 0.3081 | Train Acc: 0.8888 | Train F1:  0.9018 | Val Acc:   0.8114 | Val F1:    0.8347\n",
      "Epoch 62 | Train Loss: 0.3064 | Train Acc: 0.8877 | Train F1:  0.9013 | Val Acc:   0.8133 | Val F1:    0.8372\n",
      "Epoch 63 | Train Loss: 0.3066 | Train Acc: 0.8903 | Train F1:  0.9028 | Val Acc:   0.8144 | Val F1:    0.8366\n",
      "Epoch 64 | Train Loss: 0.3063 | Train Acc: 0.8894 | Train F1:  0.9026 | Val Acc:   0.8150 | Val F1:    0.8382\n",
      "Epoch 65 | Train Loss: 0.3050 | Train Acc: 0.8878 | Train F1:  0.9018 | Val Acc:   0.8156 | Val F1:    0.8400\n",
      "Epoch 66 | Train Loss: 0.3062 | Train Acc: 0.8891 | Train F1:  0.9023 | Val Acc:   0.8126 | Val F1:    0.8364\n",
      "Epoch 67 | Train Loss: 0.3050 | Train Acc: 0.8863 | Train F1:  0.9009 | Val Acc:   0.8148 | Val F1:    0.8403\n",
      "Epoch 68 | Train Loss: 0.3033 | Train Acc: 0.8924 | Train F1:  0.9041 | Val Acc:   0.8124 | Val F1:    0.8334\n",
      "Epoch 69 | Train Loss: 0.3036 | Train Acc: 0.8899 | Train F1:  0.9030 | Val Acc:   0.8128 | Val F1:    0.8365\n",
      "Epoch 70 | Train Loss: 0.3040 | Train Acc: 0.8895 | Train F1:  0.9030 | Val Acc:   0.8137 | Val F1:    0.8377\n",
      "Epoch 71 | Train Loss: 0.3021 | Train Acc: 0.8878 | Train F1:  0.9019 | Val Acc:   0.8151 | Val F1:    0.8396\n",
      "Epoch 72 | Train Loss: 0.3032 | Train Acc: 0.8903 | Train F1:  0.9033 | Val Acc:   0.8143 | Val F1:    0.8375\n",
      "Epoch 73 | Train Loss: 0.3011 | Train Acc: 0.8918 | Train F1:  0.9044 | Val Acc:   0.8137 | Val F1:    0.8364\n",
      "Epoch 74 | Train Loss: 0.3020 | Train Acc: 0.8918 | Train F1:  0.9042 | Val Acc:   0.8140 | Val F1:    0.8363\n",
      "Epoch 75 | Train Loss: 0.3043 | Train Acc: 0.8887 | Train F1:  0.9025 | Val Acc:   0.8149 | Val F1:    0.8394\n",
      "Epoch 76 | Train Loss: 0.3030 | Train Acc: 0.8866 | Train F1:  0.9013 | Val Acc:   0.8134 | Val F1:    0.8392\n",
      "Epoch 77 | Train Loss: 0.3026 | Train Acc: 0.8917 | Train F1:  0.9043 | Val Acc:   0.8145 | Val F1:    0.8373\n",
      "Epoch 78 | Train Loss: 0.3007 | Train Acc: 0.8917 | Train F1:  0.9041 | Val Acc:   0.8133 | Val F1:    0.8357\n",
      "Epoch 79 | Train Loss: 0.3017 | Train Acc: 0.8890 | Train F1:  0.9028 | Val Acc:   0.8151 | Val F1:    0.8397\n",
      "Early stopping at epoch 80.\n",
      "================================\n",
      "\n",
      "Best Model Stats:\n",
      "Train Loss: 0.3050 | Train Acc: 0.8878 | Train F1:  0.9018 | Val Acc:   0.8156 | Val F1:    0.8400\n"
     ]
    }
   ],
   "source": [
    "best_model_stats = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "training_history = []\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train_loss = train()\n",
    "    train_acc, train_f1 = test(train_loader)\n",
    "    val_acc, val_f1 = test(val_loader)\n",
    "    scheduler.step(val_acc)\n",
    "        \n",
    "    training_history.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_f1\": val_f1,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']   \n",
    "    })\n",
    "\n",
    "    if val_acc > best_model_stats[3]:\n",
    "        best_model_stats = [train_loss,\n",
    "                            train_acc, train_f1, val_acc, val_f1]\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"/kaggle/working/best_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | \"\n",
    "            f\"Train F1:  {train_f1:.4f} | \"\n",
    "            f\"Val Acc:   {val_acc:.4f} | \"\n",
    "            f\"Val F1:    {val_f1:.4f}\")\n",
    "\n",
    "\n",
    "history_df = pd.DataFrame(training_history)\n",
    "history_df.to_csv(\"/kaggle/working/training_history.csv\", index=False)\n",
    "    \n",
    "print(\"================================\\n\")\n",
    "print(\"Best Model Stats:\")\n",
    "print(f\"Train Loss: {best_model_stats[0]:.4f} | \"\n",
    "        f\"Train Acc: {best_model_stats[1]:.4f} | \"\n",
    "        f\"Train F1:  {best_model_stats[2]:.4f} | \"\n",
    "        f\"Val Acc:   {best_model_stats[3]:.4f} | \"\n",
    "        f\"Val F1:    {best_model_stats[4]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9382645,
     "isSourceIdPinned": true,
     "sourceId": 14843229,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15032.419137,
   "end_time": "2026-02-16T22:09:23.240564",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-16T17:58:50.821427",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
